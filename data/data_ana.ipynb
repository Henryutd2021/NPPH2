{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "# Base URL with placeholders for date and product\n",
    "base_url = (\"https://apim.misoenergy.org/pricing/v1/day-ahead/{date}/\"\n",
    "            \"asm-expost?pageNumber=1&product={product}\")\n",
    "\n",
    "# Request headers with your subscription key\n",
    "headers = {\n",
    "    'Cache-Control': 'no-cache',\n",
    "    'Ocp-Apim-Subscription-Key': 'your key',\n",
    "}\n",
    "\n",
    "# List of products to retrieve\n",
    "products = [\"Regulation\", \"Spin\", \"Supplemental\",\"STR\", \"Ramp-up\", \"Ramp-down\"]\n",
    "\n",
    "# Define the date range for the year 2024 (2024 is a leap year)\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "\n",
    "# Loop over each product one by one\n",
    "for product in products:\n",
    "    print(f\"Downloading data for {product}...\")\n",
    "    product_data = []  # List to store data for the current product\n",
    "    \n",
    "    # Loop through every day in 2024 for the current product\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "        # Format the URL with the current date and product\n",
    "        url = base_url.format(date=date_str, product=product)\n",
    "        \n",
    "        # Make the GET request\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            json_response = response.json()\n",
    "            for entry in json_response.get(\"data\", []):\n",
    "                # Extract the \"end\" value from the nested timeInterval dictionary\n",
    "                if \"timeInterval\" in entry and \"end\" in entry[\"timeInterval\"]:\n",
    "                    entry[\"end\"] = entry[\"timeInterval\"][\"end\"]\n",
    "                else:\n",
    "                    entry[\"end\"] = None\n",
    "                \n",
    "                # Add the query date to the record for reference\n",
    "                entry[\"query_date\"] = date_str\n",
    "                \n",
    "                # Append the record to the product-specific list\n",
    "                product_data.append(entry)\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for {date_str} and product {product}\")\n",
    "        \n",
    "        # Delay to ensure we remain below 100 API calls per minute\n",
    "        # time.sleep(0.7)\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    # Convert the collected data for the product into a DataFrame\n",
    "    df_product = pd.DataFrame(product_data)\n",
    "    \n",
    "    # Optionally convert the 'end' column to datetime for proper handling\n",
    "    df_product['end'] = pd.to_datetime(df_product['end'])\n",
    "    \n",
    "    # Pivot the DataFrame so that the index is the \"end\" time and the columns are zones (with mcp as the values)\n",
    "    pivot_table = df_product.pivot_table(index='end', columns='zone', values='mcp', aggfunc='first')\n",
    "    \n",
    "    # Save the pivoted data for the current product to a CSV file\n",
    "    filename = f\"2024_{product}_data.csv\"\n",
    "    pivot_table.to_csv(filename)\n",
    "    print(f\"Data for {product} saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file, ensuring the 'end' column is parsed as datetime and set as the index.\n",
    "for products in [\"Regulation\", \"Spin\", \"Supplemental\", \"STR\", \"Ramp-up\", \"Ramp-down\"]:\n",
    "\n",
    "    print(f'checking {products} file')\n",
    "\n",
    "    filename = f\"2024_{products}_data.csv\"  # Replace with your file name if needed\n",
    "    df = pd.read_csv(filename, parse_dates=['end'], index_col='end')\n",
    "\n",
    "    # Ensure the index is sorted\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Create a complete hourly date range from the earliest to the latest 'end' timestamp\n",
    "    full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "\n",
    "    # Identify missing hours by checking which expected timestamps are not in the dataframe index\n",
    "    missing_hours = full_range.difference(df.index)\n",
    "\n",
    "    print(\"Missing Hours:\")\n",
    "    print(missing_hours)\n",
    "\n",
    "    # Identify duplicate hours by checking if any index value appears more than once\n",
    "    duplicates = df.index[df.index.duplicated(keep=False)].unique()\n",
    "\n",
    "    print(\"\\nDuplicate Hours:\")\n",
    "    print(duplicates)\n",
    "\n",
    "    # Optionally, display detailed rows for the duplicate hours\n",
    "    duplicate_details = df[df.index.duplicated(keep=False)]\n",
    "    print(\"\\nDetails of Duplicate Rows:\")\n",
    "    print(duplicate_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridstatusio import GridStatusClient\n",
    "\n",
    "client = GridStatusClient(api_key=\"your key\")\n",
    "\n",
    "name = \"pjm_as_market_results_real_time\"\n",
    "\n",
    "df = client.get_dataset(\n",
    "    dataset=name,\n",
    "    start=\"2024-01-01 00:00\",\n",
    "    end=\"2025-01-01 00:00\",\n",
    "    timezone=\"US/Eastern\",\n",
    "    filter_column=\"location\",\n",
    "    filter_value=\"MHK VL\"\n",
    ")\n",
    "df.to_csv(f\"{name}_2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_iso_data(config):\n",
    "    \"\"\"\n",
    "    Processes ancillary service price data for a given ISO based on configuration.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A dictionary containing ISO-specific configuration:\n",
    "            iso_name (str): Name of the ISO (for messages).\n",
    "            filename (str): Path to the input CSV or Excel file.\n",
    "            file_type (str): 'csv' or 'excel' (optional, detected from extension if None).\n",
    "            timezone (str): IANA timezone name (e.g., 'US/Central').\n",
    "            time_col (str): Name of the source local time column.\n",
    "            data_format (str): 'wide' or 'long'.\n",
    "            filter_col (str | None): Column name for filtering, or None.\n",
    "            filter_val (str | None): Value to filter for in filter_col, or None.\n",
    "            avg_zones (bool): If True and format is 'wide', average across all zones.\n",
    "            zone_col (str | None): Name of the zone column.\n",
    "            resample_freq (str | None): Pandas frequency string for resampling (e.g., 'H'), or None.\n",
    "            resample_agg (str | None): Aggregation method for resampling ('mean', 'sum', etc.), or None.\n",
    "            value_cols (list): List of original columns needed for processing.\n",
    "            service_type_col_long (str | None): Col name for service type ('long' format).\n",
    "            value_col_long (str | None): Col name for numerical value ('long' format).\n",
    "            rename_map (dict): Dictionary mapping original AS names/types to abbreviations.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Processed DataFrame or None if processing fails.\n",
    "    \"\"\"\n",
    "    iso_name = config['iso_name']\n",
    "    filename = config['filename']\n",
    "    file_type = config.get('file_type') # Optional config parameter\n",
    "\n",
    "    print(f\"--- Processing {iso_name} data from {filename} ---\")\n",
    "\n",
    "    try:\n",
    "        # --- Load Data ---\n",
    "        if file_type == 'excel' or filename.endswith(('.xlsx', '.xls')):\n",
    "            print(f\"Reading Excel file: {filename}\")\n",
    "            df = pd.read_excel(filename)\n",
    "        elif file_type == 'csv' or filename.endswith('.csv'):\n",
    "             print(f\"Reading CSV file: {filename}\")\n",
    "             df = pd.read_csv(filename)\n",
    "        else:\n",
    "            # Default to CSV if type not specified and extension unknown\n",
    "             print(f\"Assuming CSV format for: {filename}\")\n",
    "             df = pd.read_csv(filename)\n",
    "\n",
    "\n",
    "        # --- Initial Filtering ---\n",
    "        if config['filter_col'] and config['filter_val']:\n",
    "            print(f\"Filtering for {config['filter_col']} == '{config['filter_val']}'\")\n",
    "            if config['filter_col'] not in df.columns:\n",
    "                 raise ValueError(f\"Filter column '{config['filter_col']}' not found.\")\n",
    "            df_filtered = df[df[config['filter_col']] == config['filter_val']].copy()\n",
    "            if df_filtered.empty:\n",
    "                print(f\"Warning: No rows found after filtering for {config['filter_val']}. Skipping {iso_name}.\")\n",
    "                return None\n",
    "        else:\n",
    "            df_filtered = df.copy()\n",
    "\n",
    "        # --- Select Necessary Columns ---\n",
    "        required_cols = [config['time_col']] + config['value_cols']\n",
    "        if config['avg_zones'] and config['zone_col'] and config['zone_col'] not in required_cols:\n",
    "             required_cols.append(config['zone_col'])\n",
    "\n",
    "        missing_cols = [col for col in required_cols if col not in df_filtered.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        df_selected = df_filtered[required_cols].copy()\n",
    "\n",
    "        # --- Time Processing ---\n",
    "        print(f\"Processing time column '{config['time_col']}' to timezone '{config['timezone']}'\")\n",
    "        df_selected['Time_parsed'] = pd.to_datetime(df_selected[config['time_col']], errors='coerce')\n",
    "        nat_count = df_selected['Time_parsed'].isna().sum()\n",
    "        df_selected.dropna(subset=['Time_parsed'], inplace=True)\n",
    "        if df_selected.empty:\n",
    "            raise ValueError(\"DataFrame empty after time conversion and dropna.\")\n",
    "\n",
    "        parsed_dtype = df_selected['Time_parsed'].dtype\n",
    "        processed_time_col = 'Time'\n",
    "\n",
    "        if pd.api.types.is_object_dtype(parsed_dtype) and nat_count == 0:\n",
    "            time_utc_series = pd.to_datetime(df_selected['Time_parsed'], utc=True)\n",
    "            time_target_series = time_utc_series.dt.tz_convert(config['timezone'])\n",
    "            df_selected[processed_time_col] = time_target_series\n",
    "        elif pd.api.types.is_datetime64_any_dtype(parsed_dtype):\n",
    "            time_series = df_selected['Time_parsed']\n",
    "            if time_series.dt.tz is None:\n",
    "                df_selected[processed_time_col] = time_series.dt.tz_localize(config['timezone'], ambiguous='infer', nonexistent='shift_forward')\n",
    "            else:\n",
    "                df_selected[processed_time_col] = time_series.dt.tz_convert(config['timezone'])\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected dtype '{parsed_dtype}' for parsed time column.\")\n",
    "\n",
    "        # Prepare columns for next step\n",
    "        cols_for_processing = [processed_time_col] + config['value_cols']\n",
    "        # Remove time_col if it's same as processed_time_col to avoid duplicate column issues later\n",
    "        if config['time_col'] in cols_for_processing and config['time_col']!= processed_time_col:\n",
    "             pass # keep both if names different\n",
    "        elif config['time_col'] == processed_time_col:\n",
    "             cols_for_processing.remove(config['time_col']) # Avoid duplicate time columns if original name was 'Time'\n",
    "\n",
    "        df_processed_time = df_selected[list(set(cols_for_processing))].copy() # Use set to ensure unique columns\n",
    "        print(f\"Time processing successful. Dtype: {df_processed_time[processed_time_col].dtype}\")\n",
    "\n",
    "        # --- Data Structuring (Pivot/Average/Select) ---\n",
    "        # Set Time as index early for potential resampling/groupby\n",
    "        if processed_time_col in df_processed_time.columns:\n",
    "             df_processed_time.set_index(processed_time_col, inplace=True)\n",
    "        elif isinstance(df_processed_time.index, pd.DatetimeIndex):\n",
    "             pass # Already indexed correctly\n",
    "        else:\n",
    "             raise ValueError(\"Cannot find or set time index before structuring.\")\n",
    "\n",
    "        value_cols_only = [col for col in config['value_cols'] if col != config.get('service_type_col_long')] # Get only numeric value columns\n",
    "\n",
    "        if config['data_format'] == 'long':\n",
    "            print(\"Pivoting data from long to wide format...\")\n",
    "            # Need service_type and value columns from original selection\n",
    "            pivot_df = df_selected[[processed_time_col, config['service_type_col_long'], config['value_col_long']]].copy()\n",
    "            pivot_df.set_index(processed_time_col, inplace=True)\n",
    "\n",
    "            df_pivoted = pivot_df.pivot_table(\n",
    "                index=pivot_df.index, # Use existing datetime index\n",
    "                columns=config['service_type_col_long'],\n",
    "                values=config['value_col_long'],\n",
    "                aggfunc='first'\n",
    "            )\n",
    "            df_pivoted.columns.name = None\n",
    "            df_structured = df_pivoted\n",
    "            # Rename using keys from rename_map\n",
    "            df_structured = df_structured.rename(columns=config['rename_map'])\n",
    "\n",
    "        elif config['data_format'] == 'wide':\n",
    "            if config['avg_zones']:\n",
    "                print(\"Averaging values across zones...\")\n",
    "                # Group by time index and average the specified value columns\n",
    "                df_averaged = df_processed_time[value_cols_only].groupby(df_processed_time.index).mean()\n",
    "                df_structured = df_averaged\n",
    "                # Rename happens AFTER averaging\n",
    "                df_structured = df_structured.rename(columns=config['rename_map'])\n",
    "            else:\n",
    "                 print(\"Selecting and renaming columns for wide format (no averaging)...\")\n",
    "                 # Just select the value columns and rename\n",
    "                 df_structured = df_processed_time[value_cols_only].rename(columns=config['rename_map'])\n",
    "        else:\n",
    "             raise ValueError(f\"Unknown data_format: {config['data_format']}\")\n",
    "\n",
    "        # --- Optional Resampling (e.g., for ISONE 5-min to hourly) ---\n",
    "        if config.get('resample_freq') and config.get('resample_agg'):\n",
    "            freq = config['resample_freq']\n",
    "            agg_method = config['resample_agg']\n",
    "            print(f\"Resampling data to frequency '{freq}' using '{agg_method}'...\")\n",
    "            if agg_method == 'mean':\n",
    "                 df_structured = df_structured.resample(freq).mean()\n",
    "            elif agg_method == 'sum':\n",
    "                 df_structured = df_structured.resample(freq).sum()\n",
    "            # Add other aggregation methods if needed\n",
    "            else:\n",
    "                 print(f\"Warning: Unsupported resample aggregation '{agg_method}'. Using 'mean'.\")\n",
    "                 df_structured = df_structured.resample(freq).mean()\n",
    "\n",
    "\n",
    "        # Ensure columns match target names after structuring/renaming/resampling\n",
    "        final_col_names = list(config['rename_map'].values())\n",
    "        missing_final_cols = [col for col in final_col_names if col not in df_structured.columns]\n",
    "        if missing_final_cols:\n",
    "             print(f\"Warning: Columns {missing_final_cols} not found after processing/renaming.\")\n",
    "             print(f\"Available columns: {df_structured.columns.tolist()}\")\n",
    "             final_col_names = [col for col in final_col_names if col in df_structured.columns]\n",
    "             if not final_col_names:\n",
    "                  raise ValueError(\"No target columns found after processing.\")\n",
    "\n",
    "        df_renamed_final = df_structured[final_col_names] # Keep only successfully processed columns\n",
    "\n",
    "        # Check index again before reindexing\n",
    "        if not isinstance(df_renamed_final.index, pd.DatetimeIndex):\n",
    "             raise TypeError(\"Index is not a DatetimeIndex before reindexing.\")\n",
    "        print(f\"Data structured. Index timezone: {df_renamed_final.index.tz}\")\n",
    "\n",
    "        # --- Reindex, Fillna, Filter ---\n",
    "        print(\"Reindexing to full 2024 hourly range, filling NaNs, removing Feb 29...\")\n",
    "        # Always reindex to hourly for final comparison, using the target timezone\n",
    "        full_range = pd.date_range(start='2024-01-01 00:00:00', end='2024-12-31 23:00:00', freq='H', tz=config['timezone'])\n",
    "\n",
    "        # Handle duplicates (safer, though less likely after resample/groupby)\n",
    "        df_renamed_final = df_renamed_final[~df_renamed_final.index.duplicated(keep='first')]\n",
    "\n",
    "        # Calculate column means from the structured data *before* reindexing\n",
    "        column_means = df_renamed_final[final_col_names].mean()\n",
    "\n",
    "        # Reindex\n",
    "        df_reindexed = df_renamed_final.reindex(full_range)\n",
    "\n",
    "        # Fill NaNs\n",
    "        df_filled = df_reindexed.fillna(column_means)\n",
    "\n",
    "        # Remove Feb 29\n",
    "        mask = ~((df_filled.index.month == 2) & (df_filled.index.day == 29))\n",
    "        df_final_iso = df_filled[mask].copy()\n",
    "\n",
    "        # --- Final Formatting ---\n",
    "        df_final_iso.reset_index(inplace=True)\n",
    "        df_final_iso = df_final_iso.rename(columns={'index': 'Time'})\n",
    "        df_final_iso['Time'] = df_final_iso['Time'].dt.tz_localize(None) # Make naive\n",
    "\n",
    "        # Round numeric columns to 3 decimal places\n",
    "        numeric_cols_final = df_final_iso.select_dtypes(include=['number']).columns\n",
    "        df_final_iso[numeric_cols_final] = df_final_iso[numeric_cols_final].round(3)\n",
    "\n",
    "        df_final_iso = df_final_iso[['Time'] + final_col_names] # Ensure final column order\n",
    "\n",
    "        print(f\"--- {iso_name} processing complete. Final shape: {df_final_iso.shape} ---\")\n",
    "        return df_final_iso\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filename}. Skipping {iso_name}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {iso_name} data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback for debugging\n",
    "        return None\n",
    "\n",
    "# --- Configuration Dictionaries (Including ISONE) ---\n",
    "\n",
    "spp_config = {\n",
    "    'iso_name': 'SPP',\n",
    "    'filename': 'spp_as_prices_2024.csv',\n",
    "    'file_type': 'csv',\n",
    "    'timezone': 'US/Central',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': 'reserve_zone',\n",
    "    'filter_val': 'SPP',\n",
    "    'avg_zones': False, 'zone_col': None,\n",
    "    'resample_freq': None, 'resample_agg': None,\n",
    "    'value_cols': ['reg_up', 'reg_dn', 'ramp_up', 'ramp_dn', 'spin', 'supp', 'unc_up'],\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'reg_up': 'RegU', 'reg_dn': 'RegD', 'ramp_up': 'RamU', 'ramp_dn': 'RamD', 'spin': 'Spin', 'supp': 'Sup', 'unc_up': 'UncU'}\n",
    "}\n",
    "\n",
    "caiso_config = {\n",
    "    'iso_name': 'CAISO',\n",
    "    'filename': 'caiso_as_prices_2024.csv',\n",
    "    'file_type': 'csv',\n",
    "    'timezone': 'US/Pacific',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': 'region', 'filter_val': 'AS_CAISO', # Single region version\n",
    "    'avg_zones': False, 'zone_col': 'region',\n",
    "    'resample_freq': None, 'resample_agg': None,\n",
    "    'value_cols': ['regulation_up', 'regulation_down', 'spinning_reserves', 'non_spinning_reserves', 'regulation_mileage_up', 'regulation_mileage_down'],\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'regulation_up': 'RegU', 'regulation_down': 'RegD', 'spinning_reserves': 'Spin', 'non_spinning_reserves': 'NSpin', 'regulation_mileage_up': 'RMU', 'regulation_mileage_down': 'RMD'}\n",
    "}\n",
    "\n",
    "ercot_config = {\n",
    "    'iso_name': 'ERCOT',\n",
    "    'filename': 'ercot_as_prices_2024.csv',\n",
    "    'file_type': 'csv',\n",
    "    'timezone': 'US/Central',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': None, 'filter_val': None,\n",
    "    'avg_zones': False, 'zone_col': None,\n",
    "    'resample_freq': None, 'resample_agg': None,\n",
    "    'value_cols': ['regulation_up', 'regulation_down', 'responsive_reserves', 'non_spinning_reserves', 'ercot_contingency_reserve_service'],\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'regulation_up': 'RegU', 'regulation_down': 'RegD', 'responsive_reserves': 'Spin', 'non_spinning_reserves': 'NSpin', 'ercot_contingency_reserve_service': 'ECRS'}\n",
    "}\n",
    "\n",
    "pjm_config = {\n",
    "    'iso_name': 'PJM',\n",
    "    'filename': 'pjm_as_market_results_real_time_2024.csv',\n",
    "    'file_type': 'csv', # Assuming CSV despite user code saving to CSV\n",
    "    'timezone': 'US/Eastern',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'long',\n",
    "    'filter_col': 'locale', 'filter_val': 'PJM RTO Reserve Zone',\n",
    "    'avg_zones': False, 'zone_col': 'locale',\n",
    "    'resample_freq': None, 'resample_agg': None, # Assume source is hourly after pivot\n",
    "    'value_cols': ['market_clearing_price','service_type'],\n",
    "    'service_type_col_long': 'service_type', 'value_col_long': 'market_clearing_price',\n",
    "    'rename_map': {'Primary Reserve': 'Rse', 'Regulation': 'Reg', 'Synchronized Reserve': 'Syn', 'Thirty Minutes Reserve': 'TMR'}\n",
    "}\n",
    "\n",
    "nyiso_config = {\n",
    "    'iso_name': 'NYISO',\n",
    "    'filename': 'nyiso_as_prices_day_ahead_hourly_2024.csv',\n",
    "    'file_type': 'csv',\n",
    "    'timezone': 'US/Eastern',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': None, 'filter_val': None,\n",
    "    'avg_zones': True, # Average across zones\n",
    "    'zone_col': 'zone',\n",
    "    'resample_freq': None, 'resample_agg': None, # Already hourly\n",
    "    'value_cols': ['spin_reserves_10_min', 'non_spin_reserves_10_min', 'reserves_30_min', 'regulation_capacity'],\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'spin_reserves_10_min': 'Spin10', 'non_spin_reserves_10_min': 'NSpin10', 'reserves_30_min': 'Res30', 'regulation_capacity': 'RegC'}\n",
    "}\n",
    "\n",
    "isone_config = {\n",
    "    'iso_name': 'ISONE',\n",
    "    'filename': '5min_reserve_price_and_designation_2024.xlsx', # Excel file\n",
    "    'file_type': 'excel', # Specify excel type\n",
    "    'timezone': 'US/Eastern',\n",
    "    'time_col': 'local_time', # From user code\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': None, 'filter_val': None, # No filtering mentioned\n",
    "    'avg_zones': False, 'zone_col': None, # Assumed system wide\n",
    "    'resample_freq': 'H', # Resample from 5min to Hourly\n",
    "    'resample_agg': 'mean', # Aggregate using mean\n",
    "    'value_cols': ['tmsr_dollar_per_MWh', 'tmnsr_dollar_per_MWh', 'tmor_dollar_per_MWh'], # From user code\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'tmsr_dollar_per_MWh': 'Spin10', 'tmnsr_dollar_per_MWh': 'NSpin10', 'tmor_dollar_per_MWh': 'OR30'} # New abbreviations\n",
    "}\n",
    "\n",
    "# --- Example Usage ---\n",
    "# You can uncomment and run the processing for any ISO you have the file for.\n",
    "# Make sure the filenames in the config match the actual files available.\n",
    "\n",
    "# df_spp_final = process_iso_data(spp_config)\n",
    "# df_caiso_final = process_iso_data(caiso_config)\n",
    "# df_ercot_final = process_iso_data(ercot_config)\n",
    "# df_pjm_final = process_iso_data(pjm_config)\n",
    "# df_nyiso_final = process_iso_data(nyiso_config)\n",
    "\n",
    "# Process ISONE data\n",
    "print(\"\\nProcessing ISONE data...\")\n",
    "df_isone_final = process_iso_data(isone_config)\n",
    "\n",
    "if df_isone_final is not None:\n",
    "    print(\"\\nISONE Final Head:\\n\", df_isone_final.head().to_markdown(index=False))\n",
    "    # Save the ISONE results as specified in the user's code snippet\n",
    "    output_filename = \"isone_as_prices_2024.csv\"\n",
    "    df_isone_final.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nISONE processed data saved to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_service_abbreviations = {\n",
    "    'SPP': [\n",
    "        'RegU',    # Regulation Up\n",
    "        'RegD',    # Regulation Down\n",
    "        'RamU',    # Ramp Up\n",
    "        'RamD',    # Ramp Down\n",
    "        'Spin',    # Spinning Reserve\n",
    "        'Sup',     # Supplemental Reserve\n",
    "        'UncU'     # Uncertainty Up (may relate to ramp/reserves)\n",
    "    ],\n",
    "    'CAISO': [\n",
    "        'RegU',    # Regulation Up\n",
    "        'RegD',    # Regulation Down\n",
    "        'Spin',    # Spinning Reserves\n",
    "        'NSpin',   # Non-Spinning Reserves\n",
    "        'RMU',     # Ramp Up\n",
    "        'RMD'      # Ramp Down\n",
    "    ],\n",
    "    'ERCOT': [\n",
    "        'RegU',    # Regulation Up\n",
    "        'RegD',    # Regulation Down\n",
    "        'Spin',    # Responsive Reserves (Mapped to Spin)\n",
    "        'NSpin',   # Non-Spinning Reserves\n",
    "        'ECRS'     # ERCOT Contingency Reserve Service\n",
    "    ],\n",
    "    'PJM': [\n",
    "        'Rse',     # Primary Reserve\n",
    "        'Reg',     # Regulation\n",
    "        'Syn',     # Synchronized Reserve\n",
    "        'TMR'      # Thirty Minutes Reserve\n",
    "    ],\n",
    "    'NYISO': [\n",
    "        'Spin10',  # 10-Minute Spinning Reserves\n",
    "        'NSpin10', # 10-Minute Non-Spinning Reserves\n",
    "        'Res30',   # 30-Minute Reserves\n",
    "        'RegC'     # Regulation Capacity\n",
    "    ],\n",
    "    'ISONE': [\n",
    "        'Spin10',  # Ten Minute Spinning Reserve (TMSR)\n",
    "        'NSpin10', # Ten Minute Non-Spinning Reserve (TMNSR)\n",
    "        'OR30'     # Thirty Minute Operating Reserve (TMOR)\n",
    "    ],\n",
    "    'MISO': [\n",
    "        'RamU',    # Ramp Up\n",
    "        'RamD',    # Ramp Down\n",
    "        'Spin',    # Spinning Reserve\n",
    "        'STR',     # Short-term reserve\n",
    "        'Sup',     # Supplemental Reserve\n",
    "        'Reg'      # Regulation\n",
    "    ]\n",
    "}\n",
    "# --- Run this script again after reinstalling ---\n",
    "import pyomo.environ as pyo\n",
    "import sys\n",
    "\n",
    "print(\"--- Pyomo/Python Version Check ---\")\n",
    "try:\n",
    "    print(\"Pyomo version:\", pyo.version.version)\n",
    "except AttributeError:\n",
    "    print(\"Could not retrieve Pyomo version (might be very old or installation issue).\")\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"--- Testing pyo.abs ---\")\n",
    "\n",
    "try:\n",
    "    # Try to access the attribute\n",
    "    abs_func = pyo.abs\n",
    "    print(\"Successfully accessed pyo.abs.\")\n",
    "\n",
    "    # Try to use it in a minimal model\n",
    "    m = pyo.ConcreteModel()\n",
    "    m.x = pyo.Var(bounds=(-5, 5))\n",
    "    # Use the accessed function\n",
    "    m.c = pyo.Constraint(expr=abs_func(m.x) <= 3)\n",
    "    print(\"Successfully created constraint using pyo.abs.\")\n",
    "    # m.pprint() # Optional: print model structure\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"FAILED: Caught AttributeError: {e}\")\n",
    "    print(\"This confirms pyo.abs is not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"FAILED: Caught other exception: {e}\")\n",
    "\n",
    "print(\"--- Checking pyo attributes (sample) ---\")\n",
    "# See what attributes *are* available\n",
    "try:\n",
    "    attributes = dir(pyo)\n",
    "    print(\"Found 'abs' in dir(pyo):\", 'abs' in attributes)\n",
    "    print(\"Sample attributes:\", attributes[50:70]) # Print a sample\n",
    "except Exception as e:\n",
    "    print(f\"Could not list attributes: {e}\")\n",
    "# --- End of script ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "# Base URL with placeholders for date and product\n",
    "base_url = (\"https://apim.misoenergy.org/pricing/v1/day-ahead/{date}/\"\n",
    "            \"asm-expost?pageNumber=1&product={product}\")\n",
    "\n",
    "# Request headers with your subscription key\n",
    "headers = {\n",
    "    'Cache-Control': 'no-cache',\n",
    "    'Ocp-Apim-Subscription-Key': 'your key',\n",
    "}\n",
    "\n",
    "# List of products to retrieve\n",
    "products = [\"Regulation\", \"Spin\", \"Supplemental\",\"STR\", \"Ramp-up\", \"Ramp-down\"]\n",
    "\n",
    "# Define the date range for the year 2024 (2024 is a leap year)\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "\n",
    "# Loop over each product one by one\n",
    "for product in products:\n",
    "    print(f\"Downloading data for {product}...\")\n",
    "    product_data = []  # List to store data for the current product\n",
    "    \n",
    "    # Loop through every day in 2024 for the current product\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "        # Format the URL with the current date and product\n",
    "        url = base_url.format(date=date_str, product=product)\n",
    "        \n",
    "        # Make the GET request\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            json_response = response.json()\n",
    "            for entry in json_response.get(\"data\", []):\n",
    "                # Extract the \"end\" value from the nested timeInterval dictionary\n",
    "                if \"timeInterval\" in entry and \"end\" in entry[\"timeInterval\"]:\n",
    "                    entry[\"end\"] = entry[\"timeInterval\"][\"end\"]\n",
    "                else:\n",
    "                    entry[\"end\"] = None\n",
    "                \n",
    "                # Add the query date to the record for reference\n",
    "                entry[\"query_date\"] = date_str\n",
    "                \n",
    "                # Append the record to the product-specific list\n",
    "                product_data.append(entry)\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for {date_str} and product {product}\")\n",
    "        \n",
    "        # Delay to ensure we remain below 100 API calls per minute\n",
    "        # time.sleep(0.7)\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    # Convert the collected data for the product into a DataFrame\n",
    "    df_product = pd.DataFrame(product_data)\n",
    "    \n",
    "    # Optionally convert the 'end' column to datetime for proper handling\n",
    "    df_product['end'] = pd.to_datetime(df_product['end'])\n",
    "    \n",
    "    # Pivot the DataFrame so that the index is the \"end\" time and the columns are zones (with mcp as the values)\n",
    "    pivot_table = df_product.pivot_table(index='end', columns='zone', values='mcp', aggfunc='first')\n",
    "    \n",
    "    # Save the pivoted data for the current product to a CSV file\n",
    "    filename = f\"2024_{product}_data.csv\"\n",
    "    pivot_table.to_csv(filename)\n",
    "    print(f\"Data for {product} saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file, ensuring the 'end' column is parsed as datetime and set as the index.\n",
    "for products in [\"Regulation\", \"Spin\", \"Supplemental\", \"STR\", \"Ramp-up\", \"Ramp-down\"]:\n",
    "\n",
    "    print(f'checking {products} file')\n",
    "\n",
    "    filename = f\"2024_{products}_data.csv\"  # Replace with your file name if needed\n",
    "    df = pd.read_csv(filename, parse_dates=['end'], index_col='end')\n",
    "\n",
    "    # Ensure the index is sorted\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Create a complete hourly date range from the earliest to the latest 'end' timestamp\n",
    "    full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "\n",
    "    # Identify missing hours by checking which expected timestamps are not in the dataframe index\n",
    "    missing_hours = full_range.difference(df.index)\n",
    "\n",
    "    print(\"Missing Hours:\")\n",
    "    print(missing_hours)\n",
    "\n",
    "    # Identify duplicate hours by checking if any index value appears more than once\n",
    "    duplicates = df.index[df.index.duplicated(keep=False)].unique()\n",
    "\n",
    "    print(\"\\nDuplicate Hours:\")\n",
    "    print(duplicates)\n",
    "\n",
    "    # Optionally, display detailed rows for the duplicate hours\n",
    "    duplicate_details = df[df.index.duplicated(keep=False)]\n",
    "    print(\"\\nDetails of Duplicate Rows:\")\n",
    "    print(duplicate_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridstatusio import GridStatusClient\n",
    "\n",
    "client = GridStatusClient(api_key=\"your key\")\n",
    "\n",
    "name = \"pjm_as_market_results_real_time\"\n",
    "\n",
    "df = client.get_dataset(\n",
    "    dataset=name,\n",
    "    start=\"2024-01-01 00:00\",\n",
    "    end=\"2025-01-01 00:00\",\n",
    "    timezone=\"US/Eastern\",\n",
    "    filter_column=\"location\",\n",
    "    filter_value=\"MHK VL\"\n",
    ")\n",
    "df.to_csv(f\"{name}_2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_iso_data(config):\n",
    "    \"\"\"\n",
    "    Processes ancillary service price data for a given ISO based on configuration.\n",
    "\n",
    "    Args:\n",
    "        config (dict): A dictionary containing ISO-specific configuration:\n",
    "            iso_name (str): Name of the ISO (for messages).\n",
    "            filename (str): Path to the input CSV or Excel file.\n",
    "            file_type (str): 'csv' or 'excel' (optional, detected from extension if None).\n",
    "            timezone (str): IANA timezone name (e.g., 'US/Central').\n",
    "            time_col (str): Name of the source local time column.\n",
    "            data_format (str): 'wide' or 'long'.\n",
    "            filter_col (str | None): Column name for filtering, or None.\n",
    "            filter_val (str | None): Value to filter for in filter_col, or None.\n",
    "            avg_zones (bool): If True and format is 'wide', average across all zones.\n",
    "            zone_col (str | None): Name of the zone column.\n",
    "            resample_freq (str | None): Pandas frequency string for resampling (e.g., 'H'), or None.\n",
    "            resample_agg (str | None): Aggregation method for resampling ('mean', 'sum', etc.), or None.\n",
    "            value_cols (list): List of original columns needed for processing.\n",
    "            service_type_col_long (str | None): Col name for service type ('long' format).\n",
    "            value_col_long (str | None): Col name for numerical value ('long' format).\n",
    "            rename_map (dict): Dictionary mapping original AS names/types to abbreviations.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Processed DataFrame or None if processing fails.\n",
    "    \"\"\"\n",
    "    iso_name = config['iso_name']\n",
    "    filename = config['filename']\n",
    "    file_type = config.get('file_type') # Optional config parameter\n",
    "\n",
    "    print(f\"--- Processing {iso_name} data from {filename} ---\")\n",
    "\n",
    "    try:\n",
    "        # --- Load Data ---\n",
    "        if file_type == 'excel' or filename.endswith(('.xlsx', '.xls')):\n",
    "            print(f\"Reading Excel file: {filename}\")\n",
    "            df = pd.read_excel(filename)\n",
    "        elif file_type == 'csv' or filename.endswith('.csv'):\n",
    "             print(f\"Reading CSV file: {filename}\")\n",
    "             df = pd.read_csv(filename)\n",
    "        else:\n",
    "            # Default to CSV if type not specified and extension unknown\n",
    "             print(f\"Assuming CSV format for: {filename}\")\n",
    "             df = pd.read_csv(filename)\n",
    "\n",
    "\n",
    "        # --- Initial Filtering ---\n",
    "        if config['filter_col'] and config['filter_val']:\n",
    "            print(f\"Filtering for {config['filter_col']} == '{config['filter_val']}'\")\n",
    "            if config['filter_col'] not in df.columns:\n",
    "                 raise ValueError(f\"Filter column '{config['filter_col']}' not found.\")\n",
    "            df_filtered = df[df[config['filter_col']] == config['filter_val']].copy()\n",
    "            if df_filtered.empty:\n",
    "                print(f\"Warning: No rows found after filtering for {config['filter_val']}. Skipping {iso_name}.\")\n",
    "                return None\n",
    "        else:\n",
    "            df_filtered = df.copy()\n",
    "\n",
    "        # --- Select Necessary Columns ---\n",
    "        required_cols = [config['time_col']] + config['value_cols']\n",
    "        if config['avg_zones'] and config['zone_col'] and config['zone_col'] not in required_cols:\n",
    "             required_cols.append(config['zone_col'])\n",
    "\n",
    "        missing_cols = [col for col in required_cols if col not in df_filtered.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        df_selected = df_filtered[required_cols].copy()\n",
    "\n",
    "        # --- Time Processing ---\n",
    "        print(f\"Processing time column '{config['time_col']}' to timezone '{config['timezone']}'\")\n",
    "        df_selected['Time_parsed'] = pd.to_datetime(df_selected[config['time_col']], errors='coerce')\n",
    "        nat_count = df_selected['Time_parsed'].isna().sum()\n",
    "        df_selected.dropna(subset=['Time_parsed'], inplace=True)\n",
    "        if df_selected.empty:\n",
    "            raise ValueError(\"DataFrame empty after time conversion and dropna.\")\n",
    "\n",
    "        parsed_dtype = df_selected['Time_parsed'].dtype\n",
    "        processed_time_col = 'Time'\n",
    "\n",
    "        if pd.api.types.is_object_dtype(parsed_dtype) and nat_count == 0:\n",
    "            time_utc_series = pd.to_datetime(df_selected['Time_parsed'], utc=True)\n",
    "            time_target_series = time_utc_series.dt.tz_convert(config['timezone'])\n",
    "            df_selected[processed_time_col] = time_target_series\n",
    "        elif pd.api.types.is_datetime64_any_dtype(parsed_dtype):\n",
    "            time_series = df_selected['Time_parsed']\n",
    "            if time_series.dt.tz is None:\n",
    "                df_selected[processed_time_col] = time_series.dt.tz_localize(config['timezone'], ambiguous='infer', nonexistent='shift_forward')\n",
    "            else:\n",
    "                df_selected[processed_time_col] = time_series.dt.tz_convert(config['timezone'])\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected dtype '{parsed_dtype}' for parsed time column.\")\n",
    "\n",
    "        # Prepare columns for next step\n",
    "        cols_for_processing = [processed_time_col] + config['value_cols']\n",
    "        # Remove time_col if it's same as processed_time_col to avoid duplicate column issues later\n",
    "        if config['time_col'] in cols_for_processing and config['time_col']!= processed_time_col:\n",
    "             pass # keep both if names different\n",
    "        elif config['time_col'] == processed_time_col:\n",
    "             cols_for_processing.remove(config['time_col']) # Avoid duplicate time columns if original name was 'Time'\n",
    "\n",
    "        df_processed_time = df_selected[list(set(cols_for_processing))].copy() # Use set to ensure unique columns\n",
    "        print(f\"Time processing successful. Dtype: {df_processed_time[processed_time_col].dtype}\")\n",
    "\n",
    "        # --- Data Structuring (Pivot/Average/Select) ---\n",
    "        # Set Time as index early for potential resampling/groupby\n",
    "        if processed_time_col in df_processed_time.columns:\n",
    "             df_processed_time.set_index(processed_time_col, inplace=True)\n",
    "        elif isinstance(df_processed_time.index, pd.DatetimeIndex):\n",
    "             pass # Already indexed correctly\n",
    "        else:\n",
    "             raise ValueError(\"Cannot find or set time index before structuring.\")\n",
    "\n",
    "        value_cols_only = [col for col in config['value_cols'] if col != config.get('service_type_col_long')] # Get only numeric value columns\n",
    "\n",
    "        if config['data_format'] == 'long':\n",
    "            print(\"Pivoting data from long to wide format...\")\n",
    "            # Need service_type and value columns from original selection\n",
    "            pivot_df = df_selected[[processed_time_col, config['service_type_col_long'], config['value_col_long']]].copy()\n",
    "            pivot_df.set_index(processed_time_col, inplace=True)\n",
    "\n",
    "            df_pivoted = pivot_df.pivot_table(\n",
    "                index=pivot_df.index, # Use existing datetime index\n",
    "                columns=config['service_type_col_long'],\n",
    "                values=config['value_col_long'],\n",
    "                aggfunc='first'\n",
    "            )\n",
    "            df_pivoted.columns.name = None\n",
    "            df_structured = df_pivoted\n",
    "            # Rename using keys from rename_map\n",
    "            df_structured = df_structured.rename(columns=config['rename_map'])\n",
    "\n",
    "        elif config['data_format'] == 'wide':\n",
    "            if config['avg_zones']:\n",
    "                print(\"Averaging values across zones...\")\n",
    "                # Group by time index and average the specified value columns\n",
    "                df_averaged = df_processed_time[value_cols_only].groupby(df_processed_time.index).mean()\n",
    "                df_structured = df_averaged\n",
    "                # Rename happens AFTER averaging\n",
    "                df_structured = df_structured.rename(columns=config['rename_map'])\n",
    "            else:\n",
    "                 print(\"Selecting and renaming columns for wide format (no averaging)...\")\n",
    "                 # Just select the value columns and rename\n",
    "                 df_structured = df_processed_time[value_cols_only].rename(columns=config['rename_map'])\n",
    "        else:\n",
    "             raise ValueError(f\"Unknown data_format: {config['data_format']}\")\n",
    "\n",
    "        # --- Optional Resampling (e.g., for ISONE 5-min to hourly) ---\n",
    "        if config.get('resample_freq') and config.get('resample_agg'):\n",
    "            freq = config['resample_freq']\n",
    "            agg_method = config['resample_agg']\n",
    "            print(f\"Resampling data to frequency '{freq}' using '{agg_method}'...\")\n",
    "            if agg_method == 'mean':\n",
    "                 df_structured = df_structured.resample(freq).mean()\n",
    "            elif agg_method == 'sum':\n",
    "                 df_structured = df_structured.resample(freq).sum()\n",
    "            # Add other aggregation methods if needed\n",
    "            else:\n",
    "                 print(f\"Warning: Unsupported resample aggregation '{agg_method}'. Using 'mean'.\")\n",
    "                 df_structured = df_structured.resample(freq).mean()\n",
    "\n",
    "\n",
    "        # Ensure columns match target names after structuring/renaming/resampling\n",
    "        final_col_names = list(config['rename_map'].values())\n",
    "        missing_final_cols = [col for col in final_col_names if col not in df_structured.columns]\n",
    "        if missing_final_cols:\n",
    "             print(f\"Warning: Columns {missing_final_cols} not found after processing/renaming.\")\n",
    "             print(f\"Available columns: {df_structured.columns.tolist()}\")\n",
    "             final_col_names = [col for col in final_col_names if col in df_structured.columns]\n",
    "             if not final_col_names:\n",
    "                  raise ValueError(\"No target columns found after processing.\")\n",
    "\n",
    "        df_renamed_final = df_structured[final_col_names] # Keep only successfully processed columns\n",
    "\n",
    "        # Check index again before reindexing\n",
    "        if not isinstance(df_renamed_final.index, pd.DatetimeIndex):\n",
    "             raise TypeError(\"Index is not a DatetimeIndex before reindexing.\")\n",
    "        print(f\"Data structured. Index timezone: {df_renamed_final.index.tz}\")\n",
    "\n",
    "        # --- Reindex, Fillna, Filter ---\n",
    "        print(\"Reindexing to full 2024 hourly range, filling NaNs, removing Feb 29...\")\n",
    "        # Always reindex to hourly for final comparison, using the target timezone\n",
    "        full_range = pd.date_range(start='2024-01-01 00:00:00', end='2024-12-31 23:00:00', freq='H', tz=config['timezone'])\n",
    "\n",
    "        # Handle duplicates (safer, though less likely after resample/groupby)\n",
    "        df_renamed_final = df_renamed_final[~df_renamed_final.index.duplicated(keep='first')]\n",
    "\n",
    "        # Calculate column means from the structured data *before* reindexing\n",
    "        column_means = df_renamed_final[final_col_names].mean()\n",
    "\n",
    "        # Reindex\n",
    "        df_reindexed = df_renamed_final.reindex(full_range)\n",
    "\n",
    "        # Fill NaNs\n",
    "        df_filled = df_reindexed.fillna(column_means)\n",
    "\n",
    "        # Remove Feb 29\n",
    "        mask = ~((df_filled.index.month == 2) & (df_filled.index.day == 29))\n",
    "        df_final_iso = df_filled[mask].copy()\n",
    "\n",
    "        # --- Final Formatting ---\n",
    "        df_final_iso.reset_index(inplace=True)\n",
    "        df_final_iso = df_final_iso.rename(columns={'index': 'Time'})\n",
    "        df_final_iso['Time'] = df_final_iso['Time'].dt.tz_localize(None) # Make naive\n",
    "\n",
    "        # Round numeric columns to 3 decimal places\n",
    "        numeric_cols_final = df_final_iso.select_dtypes(include=['number']).columns\n",
    "        df_final_iso[numeric_cols_final] = df_final_iso[numeric_cols_final].round(3)\n",
    "\n",
    "        df_final_iso = df_final_iso[['Time'] + final_col_names] # Ensure final column order\n",
    "\n",
    "        print(f\"--- {iso_name} processing complete. Final shape: {df_final_iso.shape} ---\")\n",
    "        return df_final_iso\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filename}. Skipping {iso_name}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {iso_name} data: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback for debugging\n",
    "        return None\n",
    "\n",
    "# --- Configuration Dictionaries (Including ISONE) ---\n",
    "\n",
    "spp_config = {\n",
    "    'iso_name': 'SPP',\n",
    "    'filename': 'spp_as_prices_2024.csv',\n",
    "    'file_type': 'csv',\n",
    "    'timezone': 'US/Central',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': 'reserve_zone',\n",
    "    'filter_val': 'SPP',\n",
    "    'avg_zones': False, 'zone_col': None,\n",
    "    'resample_freq': None, 'resample_agg': None,\n",
    "    'value_cols': ['reg_up', 'reg_dn', 'ramp_up', 'ramp_dn', 'spin', 'supp', 'unc_up'],\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'reg_up': 'RegU', 'reg_dn': 'RegD', 'ramp_up': 'RamU', 'ramp_dn': 'RamD', 'spin': 'Spin', 'supp': 'Sup', 'unc_up': 'UncU'}\n",
    "}\n",
    "\n",
    "caiso_config = {\n",
    "    'iso_name': 'CAISO',\n",
    "    'filename': 'caiso_as_prices_2024.csv',\n",
    "    'file_type': 'csv',\n",
    "    'timezone': 'US/Pacific',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': 'region', 'filter_val': 'AS_CAISO', # Single region version\n",
    "    'avg_zones': False, 'zone_col': 'region',\n",
    "    'resample_freq': None, 'resample_agg': None,\n",
    "    'value_cols': ['regulation_up', 'regulation_down', 'spinning_reserves', 'non_spinning_reserves', 'regulation_mileage_up', 'regulation_mileage_down'],\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'regulation_up': 'RegU', 'regulation_down': 'RegD', 'spinning_reserves': 'Spin', 'non_spinning_reserves': 'NSpin', 'regulation_mileage_up': 'RMU', 'regulation_mileage_down': 'RMD'}\n",
    "}\n",
    "\n",
    "ercot_config = {\n",
    "    'iso_name': 'ERCOT',\n",
    "    'filename': 'ercot_as_prices_2024.csv',\n",
    "    'file_type': 'csv',\n",
    "    'timezone': 'US/Central',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': None, 'filter_val': None,\n",
    "    'avg_zones': False, 'zone_col': None,\n",
    "    'resample_freq': None, 'resample_agg': None,\n",
    "    'value_cols': ['regulation_up', 'regulation_down', 'responsive_reserves', 'non_spinning_reserves', 'ercot_contingency_reserve_service'],\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'regulation_up': 'RegU', 'regulation_down': 'RegD', 'responsive_reserves': 'Spin', 'non_spinning_reserves': 'NSpin', 'ercot_contingency_reserve_service': 'ECRS'}\n",
    "}\n",
    "\n",
    "pjm_config = {\n",
    "    'iso_name': 'PJM',\n",
    "    'filename': 'pjm_as_market_results_real_time_2024.csv',\n",
    "    'file_type': 'csv', # Assuming CSV despite user code saving to CSV\n",
    "    'timezone': 'US/Eastern',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'long',\n",
    "    'filter_col': 'locale', 'filter_val': 'PJM RTO Reserve Zone',\n",
    "    'avg_zones': False, 'zone_col': 'locale',\n",
    "    'resample_freq': None, 'resample_agg': None, # Assume source is hourly after pivot\n",
    "    'value_cols': ['market_clearing_price','service_type'],\n",
    "    'service_type_col_long': 'service_type', 'value_col_long': 'market_clearing_price',\n",
    "    'rename_map': {'Primary Reserve': 'Rse', 'Regulation': 'Reg', 'Synchronized Reserve': 'Syn', 'Thirty Minutes Reserve': 'TMR'}\n",
    "}\n",
    "\n",
    "nyiso_config = {\n",
    "    'iso_name': 'NYISO',\n",
    "    'filename': 'nyiso_as_prices_day_ahead_hourly_2024.csv',\n",
    "    'file_type': 'csv',\n",
    "    'timezone': 'US/Eastern',\n",
    "    'time_col': 'interval_start_local',\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': None, 'filter_val': None,\n",
    "    'avg_zones': True, # Average across zones\n",
    "    'zone_col': 'zone',\n",
    "    'resample_freq': None, 'resample_agg': None, # Already hourly\n",
    "    'value_cols': ['spin_reserves_10_min', 'non_spin_reserves_10_min', 'reserves_30_min', 'regulation_capacity'],\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'spin_reserves_10_min': 'Spin10', 'non_spin_reserves_10_min': 'NSpin10', 'reserves_30_min': 'Res30', 'regulation_capacity': 'RegC'}\n",
    "}\n",
    "\n",
    "isone_config = {\n",
    "    'iso_name': 'ISONE',\n",
    "    'filename': '5min_reserve_price_and_designation_2024.xlsx', # Excel file\n",
    "    'file_type': 'excel', # Specify excel type\n",
    "    'timezone': 'US/Eastern',\n",
    "    'time_col': 'local_time', # From user code\n",
    "    'data_format': 'wide',\n",
    "    'filter_col': None, 'filter_val': None, # No filtering mentioned\n",
    "    'avg_zones': False, 'zone_col': None, # Assumed system wide\n",
    "    'resample_freq': 'H', # Resample from 5min to Hourly\n",
    "    'resample_agg': 'mean', # Aggregate using mean\n",
    "    'value_cols': ['tmsr_dollar_per_MWh', 'tmnsr_dollar_per_MWh', 'tmor_dollar_per_MWh'], # From user code\n",
    "    'service_type_col_long': None, 'value_col_long': None,\n",
    "    'rename_map': {'tmsr_dollar_per_MWh': 'Spin10', 'tmnsr_dollar_per_MWh': 'NSpin10', 'tmor_dollar_per_MWh': 'OR30'} # New abbreviations\n",
    "}\n",
    "\n",
    "# --- Example Usage ---\n",
    "# You can uncomment and run the processing for any ISO you have the file for.\n",
    "# Make sure the filenames in the config match the actual files available.\n",
    "\n",
    "# df_spp_final = process_iso_data(spp_config)\n",
    "# df_caiso_final = process_iso_data(caiso_config)\n",
    "# df_ercot_final = process_iso_data(ercot_config)\n",
    "# df_pjm_final = process_iso_data(pjm_config)\n",
    "# df_nyiso_final = process_iso_data(nyiso_config)\n",
    "\n",
    "# Process ISONE data\n",
    "print(\"\\nProcessing ISONE data...\")\n",
    "df_isone_final = process_iso_data(isone_config)\n",
    "\n",
    "if df_isone_final is not None:\n",
    "    print(\"\\nISONE Final Head:\\n\", df_isone_final.head().to_markdown(index=False))\n",
    "    # Save the ISONE results as specified in the user's code snippet\n",
    "    output_filename = \"isone_as_prices_2024.csv\"\n",
    "    df_isone_final.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nISONE processed data saved to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_service_abbreviations = {\n",
    "    'SPP': [\n",
    "        'RegU',    # Regulation Up\n",
    "        'RegD',    # Regulation Down\n",
    "        'RamU',    # Ramp Up\n",
    "        'RamD',    # Ramp Down\n",
    "        'Spin',    # Spinning Reserve\n",
    "        'Sup',     # Supplemental Reserve\n",
    "        'UncU'     # Uncertainty Up (may relate to ramp/reserves)\n",
    "    ],\n",
    "    'CAISO': [\n",
    "        'RegU',    # Regulation Up\n",
    "        'RegD',    # Regulation Down\n",
    "        'Spin',    # Spinning Reserves\n",
    "        'NSpin',   # Non-Spinning Reserves\n",
    "        'RMU',     # Ramp Up\n",
    "        'RMD'      # Ramp Down\n",
    "    ],\n",
    "    'ERCOT': [\n",
    "        'RegU',    # Regulation Up\n",
    "        'RegD',    # Regulation Down\n",
    "        'Spin',    # Responsive Reserves (Mapped to Spin)\n",
    "        'NSpin',   # Non-Spinning Reserves\n",
    "        'ECRS'     # ERCOT Contingency Reserve Service\n",
    "    ],\n",
    "    'PJM': [\n",
    "        'Rse',     # Primary Reserve\n",
    "        'Reg',     # Regulation\n",
    "        'Syn',     # Synchronized Reserve\n",
    "        'TMR'      # Thirty Minutes Reserve\n",
    "    ],\n",
    "    'NYISO': [\n",
    "        'Spin10',  # 10-Minute Spinning Reserves\n",
    "        'NSpin10', # 10-Minute Non-Spinning Reserves\n",
    "        'Res30',   # 30-Minute Reserves\n",
    "        'RegC'     # Regulation Capacity\n",
    "    ],\n",
    "    'ISONE': [\n",
    "        'Spin10',  # Ten Minute Spinning Reserve (TMSR)\n",
    "        'NSpin10', # Ten Minute Non-Spinning Reserve (TMNSR)\n",
    "        'OR30'     # Thirty Minute Operating Reserve (TMOR)\n",
    "    ],\n",
    "    'MISO': [\n",
    "        'RamU',    # Ramp Up\n",
    "        'RamD',    # Ramp Down\n",
    "        'Spin',    # Spinning Reserve\n",
    "        'STR',     # Short-term reserve\n",
    "        'Sup',     # Supplemental Reserve\n",
    "        'Reg'      # Regulation\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('price_data_transfer.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Special mappings for certain ISOs\n",
    "special_mappings = {\n",
    "    'PJM': {'Reg': ['RegUp', 'RegDown']},\n",
    "    'MISO': {'Reg': ['RegUp', 'RegDown']},\n",
    "    'NYISO': {'RegC': ['RegUp', 'RegDown']}\n",
    "}\n",
    "\n",
    "def process_iso_files():\n",
    "    \"\"\"\n",
    "    Process all ISO price files and update the corresponding hourly data files\n",
    "    \"\"\"\n",
    "    # Source directory for ISO AS price files\n",
    "    iso_dir = './ISOs'\n",
    "    # Target directory containing ISO folders with hourly data files\n",
    "    target_base_dir = '../input/hourly_data'\n",
    "    \n",
    "    # Get all AS price files\n",
    "    iso_files = glob.glob(os.path.join(iso_dir, '*_as_prices_2024.csv'))\n",
    "    \n",
    "    if not iso_files:\n",
    "        logging.error(f\"No ISO AS price files found in {iso_dir}\")\n",
    "        return\n",
    "    \n",
    "    for iso_file in iso_files:\n",
    "        # Extract ISO name from filename\n",
    "        file_name = os.path.basename(iso_file)\n",
    "        iso_name = file_name.split('_')[0].upper()\n",
    "        \n",
    "        logging.info(f\"Processing {iso_name} from file {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the ISO AS price data\n",
    "            iso_data = pd.read_csv(iso_file)\n",
    "            \n",
    "            # Get the target file path\n",
    "            target_dir = os.path.join(target_base_dir, iso_name)\n",
    "            target_file = os.path.join(target_dir, 'Price_ANS_hourly.csv')\n",
    "            \n",
    "            if not os.path.exists(target_file):\n",
    "                logging.error(f\"Target file not found: {target_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Read the target hourly data file\n",
    "            target_data = pd.read_csv(target_file)\n",
    "            \n",
    "            # Process and update the target data\n",
    "            target_data = update_target_data(iso_name, iso_data, target_data)\n",
    "            \n",
    "            # Save the updated data\n",
    "            target_data.to_csv(target_file, index=False)\n",
    "            logging.info(f\"Updated {target_file} successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {iso_name}: {str(e)}\")\n",
    "\n",
    "def update_target_data(iso_name, source_data, target_data):\n",
    "    \"\"\"\n",
    "    Update target data with prices from source data, matching timestamps while ignoring year differences\n",
    "    \n",
    "    Args:\n",
    "        iso_name: Name of the ISO\n",
    "        source_data: DataFrame containing the source price data (2024)\n",
    "        target_data: DataFrame to be updated (2023)\n",
    "        \n",
    "    Returns:\n",
    "        Updated target DataFrame\n",
    "    \"\"\"\n",
    "    # Get the first column name (timestamp column)\n",
    "    time_col = source_data.columns[0]\n",
    "    target_time_col = target_data.columns[0]\n",
    "    \n",
    "    # Convert timestamps to datetime objects\n",
    "    try:\n",
    "        if pd.api.types.is_object_dtype(source_data[time_col]):\n",
    "            source_data[time_col] = pd.to_datetime(source_data[time_col])\n",
    "        if pd.api.types.is_object_dtype(target_data[target_time_col]):\n",
    "            target_data[target_time_col] = pd.to_datetime(target_data[target_time_col])\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Timestamp conversion issue for {iso_name}: {str(e)}\")\n",
    "    \n",
    "    # Create matching keys based on month, day, and hour (ignoring year)\n",
    "    # This allows 2023 data to match with 2024 data\n",
    "    source_data['match_key'] = source_data[time_col].dt.strftime('%m-%d-%H')\n",
    "    target_data['match_key'] = target_data[target_time_col].dt.strftime('%m-%d-%H')\n",
    "    \n",
    "    # Set all 'loc_' columns to 0\n",
    "    loc_columns = [col for col in target_data.columns if col.startswith('loc_')]\n",
    "    for col in loc_columns:\n",
    "        logging.info(f\"Setting column {col} to 0 for {iso_name}\")\n",
    "        target_data[col] = 0\n",
    "    \n",
    "    # Get services available for this ISO\n",
    "    available_services = [col for col in source_data.columns if col != time_col and col != 'match_key']\n",
    "    \n",
    "    for service in available_services:\n",
    "        if service in iso_service_abbreviations.get(iso_name, []):\n",
    "            # Handle special mappings for PJM, MISO, and NYISO\n",
    "            if iso_name in special_mappings and service in special_mappings[iso_name]:\n",
    "                target_services = special_mappings[iso_name][service]\n",
    "                for target_service in target_services:\n",
    "                    target_col = f\"p_{target_service}_{iso_name}\"\n",
    "                    if target_col in target_data.columns:\n",
    "                        logging.info(f\"Mapping {service} to {target_col} for {iso_name}\")\n",
    "                        update_column_by_match_key(source_data, target_data, service, target_col)\n",
    "                    else:\n",
    "                        logging.warning(f\"Target column {target_col} not found in target data for {iso_name}\")\n",
    "            else:\n",
    "                # Standard mapping\n",
    "                target_col = f\"p_{service}_{iso_name}\"\n",
    "                if target_col in target_data.columns:\n",
    "                    logging.info(f\"Updating column {target_col} for {iso_name}\")\n",
    "                    update_column_by_match_key(source_data, target_data, service, target_col)\n",
    "                else:\n",
    "                    logging.warning(f\"Target column {target_col} not found in target data for {iso_name}\")\n",
    "    \n",
    "    # Clean up the temporary match_key column\n",
    "    if 'match_key' in target_data.columns:\n",
    "        target_data.drop('match_key', axis=1, inplace=True)\n",
    "    \n",
    "    return target_data\n",
    "\n",
    "def update_column_by_match_key(source_data, target_data, source_col, target_col):\n",
    "    \"\"\"\n",
    "    Update a specific column in the target DataFrame using match_key (month-day-hour)\n",
    "    \n",
    "    Args:\n",
    "        source_data: Source DataFrame\n",
    "        target_data: Target DataFrame to update\n",
    "        source_col: Column name in source data\n",
    "        target_col: Column name in target data\n",
    "    \"\"\"\n",
    "    # Create a mapping dictionary from match_key to values\n",
    "    source_map = dict(zip(source_data['match_key'], source_data[source_col]))\n",
    "    \n",
    "    # Update the target column based on matching keys\n",
    "    for idx, row in target_data.iterrows():\n",
    "        match_key = row['match_key']\n",
    "        if match_key in source_map:\n",
    "            target_data.at[idx, target_col] = source_map[match_key]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Starting ISO price data transfer process\")\n",
    "    process_iso_files()\n",
    "    logging.info(\"ISO price data transfer process completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('lmp_data_transfer.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def process_lmp_data():\n",
    "    \"\"\"\n",
    "    Process LMP data for all ISOs, calculate average across hubs, and update target files\n",
    "    \"\"\"\n",
    "    # Source directory for ISO LMP files\n",
    "    lmp_dir = './Raw/LMP'\n",
    "    # Target directory containing ISO Price_hourly.csv files\n",
    "    target_base_dir = '../input/hourly_data'\n",
    "    \n",
    "    # Get all LMP files\n",
    "    lmp_files = glob.glob(os.path.join(lmp_dir, '*_CLEANED.csv'))\n",
    "    \n",
    "    if not lmp_files:\n",
    "        logging.error(f\"No ISO LMP files found in {lmp_dir}\")\n",
    "        return\n",
    "    \n",
    "    for lmp_file in lmp_files:\n",
    "        # Extract ISO name from filename\n",
    "        file_name = os.path.basename(lmp_file)\n",
    "        iso_name = file_name.split('_')[0].upper()\n",
    "        \n",
    "        logging.info(f\"Processing LMP data for {iso_name} from file {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the ISO LMP data\n",
    "            lmp_data = pd.read_csv(lmp_file)\n",
    "            \n",
    "            # Get the target file path\n",
    "            target_dir = os.path.join(target_base_dir, iso_name)\n",
    "            target_file = os.path.join(target_dir, 'Price_hourly.csv')\n",
    "            \n",
    "            if not os.path.exists(target_file):\n",
    "                logging.error(f\"Target file not found: {target_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Read the target hourly data file\n",
    "            target_data = pd.read_csv(target_file)\n",
    "            \n",
    "            # Process LMP data and update target file\n",
    "            target_data = update_price_data(iso_name, lmp_data, target_data)\n",
    "            \n",
    "            # Save the updated data\n",
    "            target_data.to_csv(target_file, index=False)\n",
    "            logging.info(f\"Updated {target_file} successfully with average LMP data\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {iso_name} LMP data: {str(e)}\")\n",
    "\n",
    "def update_price_data(iso_name, lmp_data, target_data):\n",
    "    \"\"\"\n",
    "    Calculate average LMP across hubs and update target data\n",
    "    \n",
    "    Args:\n",
    "        iso_name: Name of the ISO\n",
    "        lmp_data: DataFrame containing the LMP data (2024)\n",
    "        target_data: Target DataFrame to be updated (2023)\n",
    "        \n",
    "    Returns:\n",
    "        Updated target DataFrame\n",
    "    \"\"\"\n",
    "    # Get the first column name (timestamp column) from both dataframes\n",
    "    lmp_time_col = lmp_data.columns[0]\n",
    "    target_time_col = target_data.columns[0]\n",
    "    \n",
    "    # Convert timestamps to datetime objects\n",
    "    try:\n",
    "        if pd.api.types.is_object_dtype(lmp_data[lmp_time_col]):\n",
    "            lmp_data[lmp_time_col] = pd.to_datetime(lmp_data[lmp_time_col])\n",
    "        if pd.api.types.is_object_dtype(target_data[target_time_col]):\n",
    "            target_data[target_time_col] = pd.to_datetime(target_data[target_time_col])\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Timestamp conversion issue for {iso_name}: {str(e)}\")\n",
    "    \n",
    "    # Create matching keys based on month, day, and hour (ignoring year)\n",
    "    lmp_data['match_key'] = lmp_data[lmp_time_col].dt.strftime('%m-%d-%H')\n",
    "    target_data['match_key'] = target_data[target_time_col].dt.strftime('%m-%d-%H')\n",
    "    \n",
    "    # Get all hub columns (exclude the timestamp column)\n",
    "    hub_columns = [col for col in lmp_data.columns if col != lmp_time_col and col != 'match_key']\n",
    "    \n",
    "    if not hub_columns:\n",
    "        logging.warning(f\"No hub columns found in LMP data for {iso_name}\")\n",
    "        return target_data\n",
    "    \n",
    "    # Calculate average LMP across all hubs for each timestamp\n",
    "    lmp_data['avg_lmp'] = lmp_data[hub_columns].mean(axis=1).round(3)\n",
    "    \n",
    "    # Create a mapping from match_key to average LMP\n",
    "    lmp_map = dict(zip(lmp_data['match_key'], lmp_data['avg_lmp']))\n",
    "    \n",
    "    # Update the Price column in target data\n",
    "    price_col = \"Price ($/MWh)\"\n",
    "    if price_col in target_data.columns:\n",
    "        logging.info(f\"Updating {price_col} with average LMP for {iso_name}\")\n",
    "        for idx, row in target_data.iterrows():\n",
    "            match_key = row['match_key']\n",
    "            if match_key in lmp_map:\n",
    "                target_data.at[idx, price_col] = lmp_map[match_key]\n",
    "    else:\n",
    "        logging.warning(f\"Price column '{price_col}' not found in target data for {iso_name}\")\n",
    "    \n",
    "    # Clean up the temporary match_key column\n",
    "    if 'match_key' in target_data.columns:\n",
    "        target_data.drop('match_key', axis=1, inplace=True)\n",
    "    \n",
    "    return target_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Starting LMP data transfer process\")\n",
    "    process_lmp_data()\n",
    "    logging.info(\"LMP data transfer process completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
